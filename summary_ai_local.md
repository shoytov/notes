Пытаяь выжать максимум из локально запущенной LLM, обнаружил, что модель **Qwen2.5 14b** неплохо справляется с задачей саммаризации текста. Решил таким образом немного автоматизировать ежедневный утренний процесс просмотра новостных материалов.

Что потребуется:

- [LM Studio](https://lmstudio.ai) - удобная GUI тулза для локального запуска моделей.  В ней надо включить на вкладке **Developer** http сервер (`ctrl+R`)
- [ai chat](https://github.com/sigoden/aichat) - консольная утилита для работы с LLM 
- аналогичный [этому](https://github.com/shoytov/dotfiles/blob/master/aichat/config.yaml) конфиг для aichat

Пока никакие MCP инструменты я не прикрутил, часть работы придется сделать "руками", а именно - открыть страницы с новостями, скопировать из них текст, создать текстовые файлы и вставить в эти файлы скопированный текст. Например, хотим сделать саммари к 3 статьям: создаем 3 файла `1.txt 2.txt 3.txt` и копируем в них текст соответственно из 1, 2, и 3 статей.

Все, теперь запускаем:

```
cat 1.txt |  aichat -m deepseek сделай саммари текста: > out1.txt && \
cat 2.txt |  aichat -m deepseek сделай саммари текста: > out2.txt && \
cat 3.txt |  aichat -m deepseek сделай саммари текста: > out3.txt
```

Ну все, теперь можем заниматься своими другими важными делами: завтркать, делать зарядку или еще что-то. Кстати, у меня этот процесс не ест все ресурсы системы, так что я спокойно могу писать код, серфить интернет или делать что-то другое на компьютере без тормозов.

После того, как команды завершит работу, ознакамливаемся с содержимым файлов саммаризации и дальше уже если самммари "зацепило" открываем статью в оригинале.

Вот [тут](https://habr.com/ru/articles/927938/) еще пример с другим инструментом автоматизации работы с текстом при помощи LLM.
